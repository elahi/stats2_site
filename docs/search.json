[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Online resources",
    "section": "",
    "text": "Useful websites related to the material:\n\nStatistical Rethinking\nRichard McElreath’s site for Statistical Rethinking:\nhttps://xcelab.net/rm/statistical-rethinking/\n\n\nStan\nStan user’s guide:\nhttps://mc-stan.org/docs/stan-users-guide/index.html\nStan case studies:\nhttps://mc-stan.org/users/documentation/case-studies.html\nUnderstand the contents of a stanfit object:\nhttps://cran.r-project.org/web/packages/rstan/vignettes/stanfit-objects.html\nTroubleshooting:\nhttps://www.alexpghayes.com/post/2018-12-24_some-things-ive-learned-about-stan/\nMulti-level models:\nhttps://discourse.mc-stan.org/t/the-simplest-varying-intercepts-slopes-model-there-is/14305\nhttps://willhipson.netlify.app/post/stan-random-slopes/varying_effects_stan/\nhttps://mlisi.xyz/post/bayesian-multilevel-models-r-stan/"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course materials",
    "section": "",
    "text": "Syllabus\nCourse intro\nCode for video 2\nComponents of a Bayesian model"
  },
  {
    "objectID": "materials.html#week-1",
    "href": "materials.html#week-1",
    "title": "Course materials",
    "section": "",
    "text": "Syllabus\nCourse intro\nCode for video 2\nComponents of a Bayesian model"
  },
  {
    "objectID": "materials.html#week-2",
    "href": "materials.html#week-2",
    "title": "Course materials",
    "section": "Week 2",
    "text": "Week 2\nSampling the posterior\nSampling practice (SR2 ch3)\n\nCode for video 3"
  },
  {
    "objectID": "code/video2.html",
    "href": "code/video2.html",
    "title": "Video lecture 2 (Garden of Forking Data)",
    "section": "",
    "text": "Book: Statistical Rethinking, 2nd Edition\nVideo lectures: https://github.com/rmcelreath/stat_rethinking_2025\nSource code for this page\nlibrary(rethinking)"
  },
  {
    "objectID": "code/video2.html#chapter-2",
    "href": "code/video2.html#chapter-2",
    "title": "Video lecture 2 (Garden of Forking Data)",
    "section": "Chapter 2",
    "text": "Chapter 2\n\nsample &lt;- c(\"W\",\"L\",\"W\",\"W\",\"W\",\"L\",\"W\",\"L\",\"W\")\nW &lt;- sum(sample==\"W\") # number of W observed\nL &lt;- sum(sample==\"L\") # number of L observed\np &lt;- c(0,0.25,0.5,0.75,1) # proportions W\nways &lt;- sapply( p , function(q) (q*4)^W * ((1-q)*4)^L )\nprob &lt;- ways/sum(ways)\ncbind( p , ways , prob )\n\n        p ways       prob\n[1,] 0.00    0 0.00000000\n[2,] 0.25   27 0.02129338\n[3,] 0.50  512 0.40378549\n[4,] 0.75  729 0.57492114\n[5,] 1.00    0 0.00000000\n\n\n\n# function to toss a globe covered p by water N times\nsim_globe &lt;- function( p=0.7 , N=9 ) {\n  sample(c(\"W\",\"L\"),size=N,prob=c(p,1-p),replace=TRUE)\n}\nsim_globe()\n\n[1] \"L\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\nreplicate(sim_globe(p=0.5,N=9),n=10)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] \"L\"  \"W\"  \"W\"  \"W\"  \"L\"  \"W\"  \"L\"  \"L\"  \"W\"  \"W\"  \n [2,] \"W\"  \"L\"  \"L\"  \"W\"  \"L\"  \"W\"  \"L\"  \"W\"  \"L\"  \"W\"  \n [3,] \"L\"  \"W\"  \"L\"  \"W\"  \"L\"  \"L\"  \"L\"  \"W\"  \"L\"  \"W\"  \n [4,] \"L\"  \"L\"  \"W\"  \"L\"  \"L\"  \"L\"  \"L\"  \"W\"  \"W\"  \"L\"  \n [5,] \"W\"  \"L\"  \"L\"  \"W\"  \"W\"  \"L\"  \"L\"  \"W\"  \"W\"  \"L\"  \n [6,] \"W\"  \"W\"  \"W\"  \"L\"  \"L\"  \"L\"  \"L\"  \"W\"  \"W\"  \"L\"  \n [7,] \"L\"  \"W\"  \"L\"  \"L\"  \"L\"  \"L\"  \"L\"  \"W\"  \"W\"  \"L\"  \n [8,] \"L\"  \"W\"  \"L\"  \"W\"  \"L\"  \"L\"  \"W\"  \"W\"  \"L\"  \"W\"  \n [9,] \"L\"  \"W\"  \"L\"  \"W\"  \"W\"  \"L\"  \"W\"  \"W\"  \"L\"  \"W\"  \n\nsim_globe( p=1 , N=11 )\n\n [1] \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\" \"W\"\n\nsum( sim_globe( p=0.5 , N=1e4 ) == \"W\" ) / 1e4\n\n[1] 0.5047\n\n# function to compute posterior distribution\ncompute_posterior &lt;- function( the_sample , poss=c(0,0.25,0.5,0.75,1) ) {\n  W &lt;- sum(the_sample==\"W\") # number of W observed\n  L &lt;- sum(the_sample==\"L\") # number of L observed\n  ways &lt;- sapply( poss , function(q) (q*4)^W * ((1-q)*4)^L )\n  post &lt;- ways/sum(ways)\n  bars &lt;- sapply( post, function(q) make_bar(q) )\n  data.frame( poss , ways , post=round(post,3) , bars )\n}\n\ncompute_posterior(sim_globe())\n\n  poss ways  post                 bars\n1 0.00    0 0.000                     \n2 0.25   27 0.021                     \n3 0.50  512 0.404 ########            \n4 0.75  729 0.575 ###########         \n5 1.00    0 0.000"
  },
  {
    "objectID": "code/video2.html#chapter-3",
    "href": "code/video2.html#chapter-3",
    "title": "Video lecture 2 (Garden of Forking Data)",
    "section": "Chapter 3",
    "text": "Chapter 3\n\npost_samples &lt;- rbeta( 1e3 , 6+1 , 3+1 )\nhead(post_samples)\n\n[1] 0.6829256 0.8597660 0.7418302 0.6421652 0.7287930 0.5303537\n\ndens( post_samples , lwd=4 , col=2 , xlab=\"proportion water\" , adj=0.1 )\ncurve( dbeta(x,6+1,3+1) , add=TRUE , lty=2 , lwd=3 )\n\n\n\n\n\n\n\n\n\n# now simulate posterior predictive distribution\npost_samples &lt;- rbeta(1e4,6+1,3+1)\npred_post &lt;- sapply( post_samples , function(p) sum(sim_globe(p,10)==\"W\"))\ntab_post &lt;- table(pred_post)\n\nmean_p &lt;- mean(post_samples)\nwater_mean_predictions &lt;- rbinom( 1e4 , size=9 , prob=mean_p )\nsimplehist( water_mean_predictions , xlab=\"number of W\" )\nfor ( i in 0.1:10.1 ) lines(c(i,i),c(0,tab_post[i+1]),lwd=4,col=4)"
  },
  {
    "objectID": "code/video2.html#chapter-2-1",
    "href": "code/video2.html#chapter-2-1",
    "title": "Video lecture 2 (Garden of Forking Data)",
    "section": "Chapter 2",
    "text": "Chapter 2\n\n## R code 2.1\nways &lt;- c( 0 , 3 , 8 , 9 , 0 )\nways/sum(ways)\n\n[1] 0.00 0.15 0.40 0.45 0.00\n\n## R code 2.2\ndbinom( 6 , size=9 , prob=0.5 )\n\n[1] 0.1640625\n\n## R code 2.3\n# define grid\np_grid &lt;- seq( from=0 , to=1 , length.out=20 )\n\n# define prior\nprior &lt;- rep( 1 , 20 )\n\n# compute likelihood at each value in grid\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid )\n\n# compute product of likelihood and prior\nunstd.posterior &lt;- likelihood * prior\n\n# standardize the posterior, so it sums to 1\nposterior &lt;- unstd.posterior / sum(unstd.posterior)\n\n## R code 2.4\nplot( p_grid , posterior , type=\"b\" ,\n    xlab=\"probability of water\" , ylab=\"posterior probability\" )\nmtext( \"20 points\" )\n\n\n\n\n\n\n\n## R code 2.5\nprior &lt;- ifelse( p_grid &lt; 0.5 , 0 , 1 )\nprior &lt;- exp( -5*abs( p_grid - 0.5 ) )\n\n## R code 2.6\nlibrary(rethinking)\nglobe.qa &lt;- quap(\n    alist(\n        W ~ dbinom( W+L ,p) ,  # binomial likelihood\n        p ~ dunif(0,1)     # uniform prior\n    ) ,\n    data=list(W=6,L=3) )\n\n# display summary of quadratic approximation\nprecis( globe.qa )\n\n       mean        sd      5.5%     94.5%\np 0.6666667 0.1571338 0.4155366 0.9177968\n\n## R code 2.7\n# analytical calculation\nW &lt;- 6\nL &lt;- 3\ncurve( dbeta( x , W+1 , L+1 ) , from=0 , to=1 )\n# quadratic approximation\ncurve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE )\n\n\n\n\n\n\n\n## R code 2.8\nn_samples &lt;- 1000\np &lt;- rep( NA , n_samples )\np[1] &lt;- 0.5\nW &lt;- 6\nL &lt;- 3\nfor ( i in 2:n_samples ) {\n    p_new &lt;- rnorm( 1 , p[i-1] , 0.1 )\n    if ( p_new &lt; 0 ) p_new &lt;- abs( p_new )\n    if ( p_new &gt; 1 ) p_new &lt;- 2 - p_new\n    q0 &lt;- dbinom( W , W+L , p[i-1] )\n    q1 &lt;- dbinom( W , W+L , p_new )\n    p[i] &lt;- ifelse( runif(1) &lt; q1/q0 , p_new , p[i-1] )\n}\n\n## R code 2.9\ndens( p , xlim=c(0,1) )\ncurve( dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE )"
  },
  {
    "objectID": "code/video2.html#chapter-3-1",
    "href": "code/video2.html#chapter-3-1",
    "title": "Video lecture 2 (Garden of Forking Data)",
    "section": "Chapter 3",
    "text": "Chapter 3\n\n## R code 3.1\nPr_Positive_Vampire &lt;- 0.95\nPr_Positive_Mortal &lt;- 0.01\nPr_Vampire &lt;- 0.001\nPr_Positive &lt;- Pr_Positive_Vampire * Pr_Vampire +\n               Pr_Positive_Mortal * ( 1 - Pr_Vampire )\n( Pr_Vampire_Positive &lt;- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive )\n\n[1] 0.08683729\n\n## R code 3.2\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )\nprob_p &lt;- rep( 1 , 1000 )\nprob_data &lt;- dbinom( 6 , size=9 , prob=p_grid )\nposterior &lt;- prob_data * prob_p\nposterior &lt;- posterior / sum(posterior)\n\n## R code 3.3\nsamples &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )\n\n## R code 3.4\nplot( samples )\n\n\n\n\n\n\n\n## R code 3.5\nlibrary(rethinking)\ndens( samples )\n\n\n\n\n\n\n\n## R code 3.6\n# add up posterior probability where p &lt; 0.5\nsum( posterior[ p_grid &lt; 0.5 ] )\n\n[1] 0.1718746\n\n## R code 3.7\nsum( samples &lt; 0.5 ) / 1e4\n\n[1] 0.1725\n\n## R code 3.8\nsum( samples &gt; 0.5 & samples &lt; 0.75 ) / 1e4\n\n[1] 0.6054\n\n## R code 3.9\nquantile( samples , 0.8 )\n\n      80% \n0.7597598 \n\n## R code 3.10\nquantile( samples , c( 0.1 , 0.9 ) )\n\n      10%       90% \n0.4454454 0.8168168 \n\n## R code 3.11\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )\nprior &lt;- rep(1,1000)\nlikelihood &lt;- dbinom( 3 , size=3 , prob=p_grid )\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\nsamples &lt;- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )\n\n## R code 3.12\nPI( samples , prob=0.5 )\n\n      25%       75% \n0.7087087 0.9299299 \n\n## R code 3.13\nHPDI( samples , prob=0.5 )\n\n     |0.5      0.5| \n0.8408408 0.9989990 \n\n## R code 3.14\np_grid[ which.max(posterior) ]\n\n[1] 1\n\n## R code 3.15\nchainmode( samples , adj=0.01 )\n\n[1] 0.9932716\n\n## R code 3.16\nmean( samples )\n\n[1] 0.799728\n\nmedian( samples )\n\n[1] 0.8418418\n\n## R code 3.17\nsum( posterior*abs( 0.5 - p_grid ) )\n\n[1] 0.3128752\n\n## R code 3.18\nloss &lt;- sapply( p_grid , function(d) sum( posterior*abs( d - p_grid ) ) )\n\n## R code 3.19\np_grid[ which.min(loss) ]\n\n[1] 0.8408408\n\n## R code 3.20\ndbinom( 0:2 , size=2 , prob=0.7 )\n\n[1] 0.09 0.42 0.49\n\n## R code 3.21\nrbinom( 1 , size=2 , prob=0.7 )\n\n[1] 1\n\n## R code 3.22\nrbinom( 10 , size=2 , prob=0.7 )\n\n [1] 2 1 1 2 2 1 1 2 1 2\n\n## R code 3.23\ndummy_w &lt;- rbinom( 1e5 , size=2 , prob=0.7 )\ntable(dummy_w)/1e5\n\ndummy_w\n      0       1       2 \n0.09040 0.41921 0.49039 \n\n## R code 3.24\ndummy_w &lt;- rbinom( 1e5 , size=9 , prob=0.7 )\nsimplehist( dummy_w , xlab=\"dummy water count\" )\n\n\n\n\n\n\n\n## R code 3.25\nw &lt;- rbinom( 1e4 , size=9 , prob=0.6 )\n\n## R code 3.26\nw &lt;- rbinom( 1e4 , size=9 , prob=samples )\n\n## R code 3.27\np_grid &lt;- seq( from=0 , to=1 , length.out=1000 )\nprior &lt;- rep( 1 , 1000 )\nlikelihood &lt;- dbinom( 6 , size=9 , prob=p_grid )\nposterior &lt;- likelihood * prior\nposterior &lt;- posterior / sum(posterior)\nset.seed(100)\nsamples &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code to accompany video lectures",
    "section": "",
    "text": "Video 2\nVideo 3\nVideo 4\nVideo 5\nVideo 6\nVideo 7\nVideo 8\nVideo 9\nVideo 10\nVideo 11\nVideo 12"
  },
  {
    "objectID": "code.html#section",
    "href": "code.html#section",
    "title": "Code to accompany video lectures",
    "section": "",
    "text": "Video 2\nVideo 3\nVideo 4\nVideo 5\nVideo 6\nVideo 7\nVideo 8\nVideo 9\nVideo 10\nVideo 11\nVideo 12"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Modeling (OCEANS 140/240)",
    "section": "",
    "text": "This course schedule is an adaptation of Richard McElreath’s 2024 course, using his Statistical Rethinking text.\n\nSchedule\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nPre-class\nSR2\nDue\n\n\n\n\n1\nTue., Jan. 7\nIntro to course; intro stats and coding review\nScience Before Statistics\n1\n\n\n\n1\nThu., Jan. 9\nComponents of a Bayesian model\nGarden of Forking Data (to 0:45)\n2\n\n\n\n2\nTue., Jan. 14\nSampling the imaginary\nGarden of Forking Data (0:45-1:16)\n3\nHW1\n\n\n2\nThu., Jan. 16\nIntro to linear models\nGeocentric Models\n4.1-4.4\n\n\n\n3\nTue., Jan. 21\nCategorical effects, polynomial regression, and splines\nCategories and Curves (to 1:14)\n4.5-4.6; 5.3\nHW2\n\n\n3\nThu., Jan. 23\nMultiple regression and intro to causal inference\nElemental Confounds\n5.1, SR 6\n\n\n\n4\nTue., Jan. 28\n(More) causal inference\nGood and Bad Controls\n6\nHW3\n\n\n4\nThu., Jan. 30\nOverfitting, regularization, model comparison\nOverfitting\n7.1, 7.3-7.6\n\n\n\n5\nTue., Feb. 4\nMarkov chain Monte Carlo\nMCMC\n8.1, 9\nHW4\n\n\n5\nThu., Feb. 6\nGeneralized linear models; Binomial regression\nModeling Events (to 1:16)\n10.2, 11.1\n\n\n\n6\nTue., Feb. 11\nConfounds; Poisson regression\nCounts and Confounds\n11.2\nHW5\n\n\n6\nThu., Feb. 13\nOrdered categorical outcomes\nOrdered Categories\n12\n\n\n\n7\nTue., Feb. 18\nMultilevel models\nMultilevel Models\n13\nHW6\n\n\n7\nThu., Feb. 20\nMultilevel models\nMultilevel Adventures\n13\n\n\n\n8\nTue., Feb. 25\nMultilevel models\nCorrelated Features\n14\nHW7\n\n\n8\nThu., Feb. 27\nContinuous categories\nGaussian Processes\n14.5\n\n\n\n9\nTue., Mar. 4\nMeasurement error\nMeasurement\n15.1\nHW8\n\n\n9\nThu., Mar. 6\nMissing data\nMissing Data\n15.2\n\n\n\n10\nTue., Mar. 11\nScientific models\nGeneralized Linear Madness\n16\nHW9\n\n\n10\nThu., Mar. 13\nCourse wrap-up\nHoroscopes\n17"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Resources",
    "section": "",
    "text": "Resources"
  },
  {
    "objectID": "materials.html#week-3",
    "href": "materials.html#week-3",
    "title": "Course materials",
    "section": "Week 3",
    "text": "Week 3\nCode for video 4\nCode for video 5"
  },
  {
    "objectID": "materials.html#week-4",
    "href": "materials.html#week-4",
    "title": "Course materials",
    "section": "Week 4",
    "text": "Week 4\nCode for video 6\nCode for video 7\nCode template for chapter 7"
  },
  {
    "objectID": "materials.html#week-5",
    "href": "materials.html#week-5",
    "title": "Course materials",
    "section": "Week 5",
    "text": "Week 5\nCode for video 8\nMarkov chain Monte Carlo"
  },
  {
    "objectID": "code/midterm_2025-template.html",
    "href": "code/midterm_2025-template.html",
    "title": "Midterm - 2025",
    "section": "",
    "text": "Load the data:\n\nlibrary(rethinking)\nlibrary(tidyverse)\n# Set the web address where R will look for files from this repository\nrepo_url &lt;- \"https://raw.githubusercontent.com/elahi/elahi.github.io/master/\"\ndat &lt;- read.csv(paste(repo_url, \"data/meps_2012_survey_data.csv\", sep = \"\"))\nglimpse(dat)\n\nRows: 72\nColumns: 9\n$ X           &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ site        &lt;chr&gt; \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\"…\n$ transect    &lt;chr&gt; \"ON0\", \"ON0\", \"ON0\", \"ON0\", \"ON1\", \"ON1\", \"ON1\", \"ON1\", \"O…\n$ quadrat     &lt;chr&gt; \"ON0_45\", \"ON0_46\", \"ON0_48\", \"ON0_49\", \"ON1_17\", \"ON1_84\"…\n$ chiton_mean &lt;dbl&gt; 0.0000000, 1.0000000, 0.0000000, 0.0000000, 1.0000000, 0.0…\n$ space       &lt;dbl&gt; 0.14213721, 0.22878590, 0.12508222, 0.24288815, 0.73034557…\n$ richness    &lt;int&gt; 10, 12, 8, 10, 10, 9, 9, 7, 8, 8, 9, 8, 10, 13, 10, 15, 13…\n$ simpsondiv  &lt;dbl&gt; 0.7004303, 0.7497803, 0.6160904, 0.7580468, 0.7172267, 0.7…\n$ urchin_mean &lt;dbl&gt; 0.6666667, 0.6666667, 0.6666667, 0.6666667, 1.0000000, 1.0…\n\n\nTransform the proportional cover of space so that we can model it using a normal distribution:\n\n# Define logit function\nlogit &lt;- function(p) log(p / (1 - p))\ndat &lt;- dat %&gt;% \n  mutate(space_logit = logit(space))\n# Plot\npar(mfrow = c(1,2))\nhist(dat$space, main = \"\", xlab = \"Space (proportion)\")\nhist(dat$space_logit, main = \"\", xlab = \"Space (logit)\")\n\n\n\n\n\n\n\n\nImportantly, the response is no longer bounded between 0 and 1. It is not important that logit(space) appears normally distributed! It just so happens that this is the case here. For now, ignore the effect of site and transect."
  },
  {
    "objectID": "code/midterm_2025-template.html#the-data",
    "href": "code/midterm_2025-template.html#the-data",
    "title": "Midterm - 2025",
    "section": "",
    "text": "Load the data:\n\nlibrary(rethinking)\nlibrary(tidyverse)\n# Set the web address where R will look for files from this repository\nrepo_url &lt;- \"https://raw.githubusercontent.com/elahi/elahi.github.io/master/\"\ndat &lt;- read.csv(paste(repo_url, \"data/meps_2012_survey_data.csv\", sep = \"\"))\nglimpse(dat)\n\nRows: 72\nColumns: 9\n$ X           &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ site        &lt;chr&gt; \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\", \"ON\"…\n$ transect    &lt;chr&gt; \"ON0\", \"ON0\", \"ON0\", \"ON0\", \"ON1\", \"ON1\", \"ON1\", \"ON1\", \"O…\n$ quadrat     &lt;chr&gt; \"ON0_45\", \"ON0_46\", \"ON0_48\", \"ON0_49\", \"ON1_17\", \"ON1_84\"…\n$ chiton_mean &lt;dbl&gt; 0.0000000, 1.0000000, 0.0000000, 0.0000000, 1.0000000, 0.0…\n$ space       &lt;dbl&gt; 0.14213721, 0.22878590, 0.12508222, 0.24288815, 0.73034557…\n$ richness    &lt;int&gt; 10, 12, 8, 10, 10, 9, 9, 7, 8, 8, 9, 8, 10, 13, 10, 15, 13…\n$ simpsondiv  &lt;dbl&gt; 0.7004303, 0.7497803, 0.6160904, 0.7580468, 0.7172267, 0.7…\n$ urchin_mean &lt;dbl&gt; 0.6666667, 0.6666667, 0.6666667, 0.6666667, 1.0000000, 1.0…\n\n\nTransform the proportional cover of space so that we can model it using a normal distribution:\n\n# Define logit function\nlogit &lt;- function(p) log(p / (1 - p))\ndat &lt;- dat %&gt;% \n  mutate(space_logit = logit(space))\n# Plot\npar(mfrow = c(1,2))\nhist(dat$space, main = \"\", xlab = \"Space (proportion)\")\nhist(dat$space_logit, main = \"\", xlab = \"Space (logit)\")\n\n\n\n\n\n\n\n\nImportantly, the response is no longer bounded between 0 and 1. It is not important that logit(space) appears normally distributed! It just so happens that this is the case here. For now, ignore the effect of site and transect."
  },
  {
    "objectID": "code/midterm_2025-template.html#suggested-workflow",
    "href": "code/midterm_2025-template.html#suggested-workflow",
    "title": "Midterm - 2025",
    "section": "Suggested workflow",
    "text": "Suggested workflow\nYou will skip the generative simulation for this exercise. Here is a suggested workflow:\n\nVisualize the data to make sense of the patterns\nStandardize the predictors\nCome up with sensible priors by plotting prior predictive distributions (see HW4-key or SR2 Fig 5.8 for example). Justify your choice of priors.\nRun the models using ulam. Diagnose your chains - what are the criteria you checked to ensure convergence? See SR2 9.4-9.5 for help.\nCompare WAIC for the models.\nInterpret in the context of the DAG and the published results.\nPlot the counterfactual effect of one predictor, holding any other predictors constant. Justify your choice of the model used for these posterior predictions. See SR2 Fig. 5.9 for example of a counterfactual plot from a multiple regression. Interpret."
  },
  {
    "objectID": "materials.html#week-6",
    "href": "materials.html#week-6",
    "title": "Course materials",
    "section": "Week 6",
    "text": "Week 6\nCode for video 9\n\nCode for video 10\n\nPoisson GLM\nPriors for Poisson\nTemplate for prior predictive distributions\nHW6"
  },
  {
    "objectID": "materials.html#week-7",
    "href": "materials.html#week-7",
    "title": "Course materials",
    "section": "Week 7",
    "text": "Week 7\nCode for video 11\nCode for video 12"
  }
]